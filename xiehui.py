import os
import re
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from sumy.parsers.plaintext import PlaintextParser
from sumy.nlp.tokenizers import Tokenizer
from sumy.summarizers.text_rank import TextRankSummarizer

# ========== å·¥å…·å‡½æ•° ==========

def generate_textrank_summary(text, sentence_count=3):
    # åˆ†å¥ï¼ˆä¸­æ–‡ï¼‰
    sentences = re.split(r"[ã€‚ï¼ï¼Ÿ!?]", text)
    sentences = [s.strip() for s in sentences if s.strip()]
    processed_text = "ã€‚".join(sentences)
    parser = PlaintextParser.from_string(processed_text, Tokenizer("chinese"))
    summarizer = TextRankSummarizer()
    summary = summarizer(parser.document, sentence_count)
    return "\n".join(str(sentence) for sentence in summary)

def safe_filename(name):
    return re.sub(r'[\\/:*?"<>|]', "_", name)

# ========== è·å–æ–°é—»åˆ—è¡¨ ==========

def fetch_news_list():
    url = "https://www.hbjzxh.org.cn/ajax/ajaxLoadModuleDom_h.jsp"
    data = {
        "cmd": "getWafNotCk_getAjaxPageModuleInfo",
        "_colId": "125",
        "_extId": "0",
        "moduleId": "1172",
        "href": "/col.jsp?m1172pageno=1&id=125",
        "newNextPage": "false",
        "needIncToVue": "false"
    }
    headers = {
        "User-Agent": "Mozilla/5.0"
    }

    response = requests.post(url, data=data, headers=headers)
    response.raise_for_status()

    match = re.search(r'"domStr"\s*:\s*"(.+?)"\s*,\s*"scripts"', response.text)
    if not match:
        raise ValueError("æœªæ‰¾åˆ° domStr å­—æ®µ")

    html_encoded = match.group(1).encode().decode("unicode_escape")
    soup = BeautifulSoup(html_encoded, "html.parser")

    base_url = "https://www.hbjzxh.org.cn"
    results = []

    for item in soup.select("div.m_news_info"):
        a_tag = item.select_one("a.article_title")
        date_span = item.select_one("span.normal_time")

        if a_tag and date_span:
            title = a_tag.get("title", "").strip()
            href = base_url + a_tag.get("href", "").strip()
            date = date_span.get_text(strip=True)
            results.append((title, href, date))

    return results

# ========== å¤„ç†å•ç¯‡æ–°é—» ==========

def process_article(title, url, date):
    print(f"ğŸ“„ æ­£åœ¨å¤„ç†ï¼šã€Š{title}ã€‹")
    resp = requests.get(url)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    # æå–æ­£æ–‡
    content_div = soup.select_one("div.richContent.richContent0")
    if not content_div:
        print("âš ï¸ æœªæ‰¾åˆ°æ­£æ–‡å†…å®¹")
        return
    full_text = content_div.get_text(separator="\n", strip=True)
    if not full_text:
        print("âš ï¸ æ­£æ–‡ä¸ºç©º")
        return

    # ä½¿ç”¨ TextRank æ‘˜è¦
    summary = generate_textrank_summary(full_text, sentence_count=3)

    # ä¸‹è½½é™„ä»¶
    attachments = []
    attach_div = soup.select_one("div.attachBox")
    if attach_div:
        for a in attach_div.select("a.attachName"):
            name = a.get_text(strip=True)
            href = a["href"]
            if href.startswith("//"):
                href = "https:" + href
            filename = safe_filename(name)
            file_path = os.path.join("attachments", filename)
            try:
                with requests.get(href, stream=True) as r:
                    with open(file_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                attachments.append((name, file_path))
            except Exception as e:
                print(f"âŒ é™„ä»¶ä¸‹è½½å¤±è´¥ï¼š{name} - {e}")

    # æ„é€ é€šçŸ¥ Markdown
    msg = f"""## {title}

ğŸ“… å‘å¸ƒæ—¶é—´ï¼š{date}
ğŸ“ **æ‘˜è¦ï¼š**  
{summary}
"""
    if attachments:
        msg += "\nğŸ“ **é™„ä»¶ï¼š**\n"
        for name, path in attachments:
            msg += f"- [{name}]({path})\n"

    # å†™å…¥æ–‡ä»¶
    safe_title = safe_filename(title)
    md_path = os.path.join("é€šçŸ¥è¾“å‡º", f"{safe_title}.md")
    with open(md_path, "w", encoding="utf-8") as f:
        f.write(msg)

    print(f"âœ… å®Œæˆï¼šã€Š{title}ã€‹")

# ========== ä¸»æ‰§è¡Œå…¥å£ ==========

if __name__ == "__main__":
    os.makedirs("attachments", exist_ok=True)
    os.makedirs("é€šçŸ¥è¾“å‡º", exist_ok=True)

    news_list = fetch_news_list()
    for title, url, date in news_list:
        try:
            process_article(title, url, date)
        except Exception as e:
            print(f"âŒ é”™è¯¯ï¼šã€Š{title}ã€‹ - {e}")
